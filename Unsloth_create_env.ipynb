{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Install the packages to a venv"
      ],
      "metadata": {
        "id": "SK4a_xqlzCK-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %cd /content/\n",
        "# !rm -rf unsloth_env"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXJO27IAusBe",
        "outputId": "cd2b7be8-957f-40d3-de2a-bf267fb49033"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "YIjcnMbAt--w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9262dd12-e325-4a67-ac68-31f5a0ffaede"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialized project `\u001b[36munsloth-env\u001b[39m` at `\u001b[36m/content/unsloth_env\u001b[39m`\n"
          ]
        }
      ],
      "source": [
        "!uv init unsloth_env --python=3.10"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd unsloth_env"
      ],
      "metadata": {
        "id": "ueyGHZlbuFth",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cd2c02a-50ff-43bd-c00c-ab994976e4ba"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/unsloth_env\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !uv add unsloth\n",
        "!uv add \"unsloth[cu124-torch260] @ git+https://github.com/unslothai/unsloth.git\" setuptools"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ks5SOs8TuXTk",
        "outputId": "6d25d439-be35-4726-c32c-46f19242fcd9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using CPython 3.10.12 interpreter at: \u001b[36m/usr/bin/python3.10\u001b[39m\n",
            "Creating virtual environment at: \u001b[36m.venv\u001b[39m\n",
            "\u001b[2K\u001b[2mResolved \u001b[1m91 packages\u001b[0m \u001b[2min 567ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mInstalled \u001b[1m81 packages\u001b[0m \u001b[2min 707ms\u001b[0m\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1maccelerate\u001b[0m\u001b[2m==1.9.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1maiohappyeyeballs\u001b[0m\u001b[2m==2.6.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1maiohttp\u001b[0m\u001b[2m==3.12.15\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1maiosignal\u001b[0m\u001b[2m==1.4.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1masync-timeout\u001b[0m\u001b[2m==5.0.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mattrs\u001b[0m\u001b[2m==25.3.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mbitsandbytes\u001b[0m\u001b[2m==0.46.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mcertifi\u001b[0m\u001b[2m==2025.7.14\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mcharset-normalizer\u001b[0m\u001b[2m==3.4.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mcut-cross-entropy\u001b[0m\u001b[2m==25.1.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mdatasets\u001b[0m\u001b[2m==3.6.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mdill\u001b[0m\u001b[2m==0.3.8\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mdocstring-parser\u001b[0m\u001b[2m==0.17.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mfilelock\u001b[0m\u001b[2m==3.18.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mfrozenlist\u001b[0m\u001b[2m==1.7.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mfsspec\u001b[0m\u001b[2m==2025.3.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mhf-transfer\u001b[0m\u001b[2m==0.1.9\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mhf-xet\u001b[0m\u001b[2m==1.1.5\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mhuggingface-hub\u001b[0m\u001b[2m==0.34.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1midna\u001b[0m\u001b[2m==3.10\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mjinja2\u001b[0m\u001b[2m==3.1.6\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mmarkdown-it-py\u001b[0m\u001b[2m==3.0.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mmarkupsafe\u001b[0m\u001b[2m==3.0.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mmdurl\u001b[0m\u001b[2m==0.1.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mmpmath\u001b[0m\u001b[2m==1.3.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mmsgspec\u001b[0m\u001b[2m==0.19.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mmultidict\u001b[0m\u001b[2m==6.6.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mmultiprocess\u001b[0m\u001b[2m==0.70.16\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnetworkx\u001b[0m\u001b[2m==3.4.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==2.2.6\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cublas-cu12\u001b[0m\u001b[2m==12.4.5.8\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-cupti-cu12\u001b[0m\u001b[2m==12.4.127\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-nvrtc-cu12\u001b[0m\u001b[2m==12.4.127\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-runtime-cu12\u001b[0m\u001b[2m==12.4.127\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cudnn-cu12\u001b[0m\u001b[2m==9.1.0.70\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cufft-cu12\u001b[0m\u001b[2m==11.2.1.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-curand-cu12\u001b[0m\u001b[2m==10.3.5.147\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cusolver-cu12\u001b[0m\u001b[2m==11.6.1.9\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cusparse-cu12\u001b[0m\u001b[2m==12.3.1.170\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cusparselt-cu12\u001b[0m\u001b[2m==0.6.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-nccl-cu12\u001b[0m\u001b[2m==2.21.5\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-nvjitlink-cu12\u001b[0m\u001b[2m==12.4.127\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-nvtx-cu12\u001b[0m\u001b[2m==12.4.127\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpackaging\u001b[0m\u001b[2m==25.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpandas\u001b[0m\u001b[2m==2.3.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpeft\u001b[0m\u001b[2m==0.16.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpillow\u001b[0m\u001b[2m==11.3.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpropcache\u001b[0m\u001b[2m==0.3.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mprotobuf\u001b[0m\u001b[2m==6.31.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpsutil\u001b[0m\u001b[2m==7.0.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpyarrow\u001b[0m\u001b[2m==21.0.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpygments\u001b[0m\u001b[2m==2.19.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpython-dateutil\u001b[0m\u001b[2m==2.9.0.post0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpytz\u001b[0m\u001b[2m==2025.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpyyaml\u001b[0m\u001b[2m==6.0.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mregex\u001b[0m\u001b[2m==2024.11.6\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mrequests\u001b[0m\u001b[2m==2.32.4\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mrich\u001b[0m\u001b[2m==14.1.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1msafetensors\u001b[0m\u001b[2m==0.5.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1msentencepiece\u001b[0m\u001b[2m==0.2.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1msetuptools\u001b[0m\u001b[2m==80.9.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mshtab\u001b[0m\u001b[2m==1.7.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1msix\u001b[0m\u001b[2m==1.17.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1msympy\u001b[0m\u001b[2m==1.13.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtokenizers\u001b[0m\u001b[2m==0.21.4\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtorch\u001b[0m\u001b[2m==2.6.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtqdm\u001b[0m\u001b[2m==4.67.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtransformers\u001b[0m\u001b[2m==4.54.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtriton\u001b[0m\u001b[2m==3.2.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtrl\u001b[0m\u001b[2m==0.20.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtypeguard\u001b[0m\u001b[2m==4.4.4\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtyping-extensions\u001b[0m\u001b[2m==4.14.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtyro\u001b[0m\u001b[2m==0.9.26\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtzdata\u001b[0m\u001b[2m==2025.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1munsloth\u001b[0m\u001b[2m==2025.7.11 (from git+https://github.com/unslothai/unsloth.git@5266ead104938c4908c7f2d2a60526555faf7e85)\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1munsloth-zoo\u001b[0m\u001b[2m==2025.7.11\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1murllib3\u001b[0m\u001b[2m==2.5.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mwheel\u001b[0m\u001b[2m==0.45.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mxformers\u001b[0m\u001b[2m==0.0.29.post3 (from https://download.pytorch.org/whl/cu124/xformers-0.0.29.post3-cp310-cp310-manylinux_2_28_x86_64.whl)\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mxxhash\u001b[0m\u001b[2m==3.5.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1myarl\u001b[0m\u001b[2m==1.20.1\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing that unsloth is indeed installed"
      ],
      "metadata": {
        "id": "3-b7Eoh6zIZM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "script = \"\"\"\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Qwen3-0.6B\",\n",
        "    max_seq_length = 2048,   # Context length - can be longer, but uses more memory\n",
        "    load_in_4bit = True,     # 4bit uses much less memory\n",
        "    load_in_8bit = False,    # A bit more accurate, uses 2x memory\n",
        "    full_finetuning = False, # We have full finetuning now!\n",
        "    # token = \"hf_...\",      # use one if using gated models\n",
        ")\n",
        "print(model)\n",
        "\"\"\"\n",
        "! uv run python -c '{script}'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8pChjnmu7J8",
        "outputId": "2022baf5-4d52-4361-e674-7512d75761f1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
            "==((====))==  Unsloth 2025.7.11: Fast Qwen3 patching. Transformers: 4.54.0.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "tokenizer_config.json: 10.5kB [00:00, 33.6MB/s]\n",
            "vocab.json: 2.78MB [00:00, 93.7MB/s]\n",
            "merges.txt: 1.67MB [00:00, 128MB/s]\n",
            "added_tokens.json: 100% 707/707 [00:00<00:00, 5.11MB/s]\n",
            "special_tokens_map.json: 100% 614/614 [00:00<00:00, 4.29MB/s]\n",
            "tokenizer.json: 100% 11.4M/11.4M [00:01<00:00, 8.26MB/s]\n",
            "chat_template.jinja: 4.91kB [00:00, 20.8MB/s]\n",
            "Qwen3ForCausalLM(\n",
            "  (model): Qwen3Model(\n",
            "    (embed_tokens): Embedding(151936, 1024, padding_idx=151654)\n",
            "    (layers): ModuleList(\n",
            "      (0): Qwen3DecoderLayer(\n",
            "        (self_attn): Qwen3Attention(\n",
            "          (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
            "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
            "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
            "          (rotary_emb): LlamaRotaryEmbedding()\n",
            "        )\n",
            "        (mlp): Qwen3MLP(\n",
            "          (gate_proj): Linear4bit(in_features=1024, out_features=3072, bias=False)\n",
            "          (up_proj): Linear4bit(in_features=1024, out_features=3072, bias=False)\n",
            "          (down_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
            "        (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
            "      )\n",
            "      (1): Qwen3DecoderLayer(\n",
            "        (self_attn): Qwen3Attention(\n",
            "          (q_proj): Linear4bit(in_features=1024, out_features=2048, bias=False)\n",
            "          (k_proj): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
            "          (v_proj): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
            "          (o_proj): Linear4bit(in_features=2048, out_features=1024, bias=False)\n",
            "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
            "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
            "          (rotary_emb): LlamaRotaryEmbedding()\n",
            "        )\n",
            "        (mlp): Qwen3MLP(\n",
            "          (gate_proj): Linear4bit(in_features=1024, out_features=3072, bias=False)\n",
            "          (up_proj): Linear4bit(in_features=1024, out_features=3072, bias=False)\n",
            "          (down_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
            "        (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
            "      )\n",
            "      (2): Qwen3DecoderLayer(\n",
            "        (self_attn): Qwen3Attention(\n",
            "          (q_proj): Linear4bit(in_features=1024, out_features=2048, bias=False)\n",
            "          (k_proj): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
            "          (v_proj): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
            "          (o_proj): Linear4bit(in_features=2048, out_features=1024, bias=False)\n",
            "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
            "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
            "          (rotary_emb): LlamaRotaryEmbedding()\n",
            "        )\n",
            "        (mlp): Qwen3MLP(\n",
            "          (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
            "          (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
            "          (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
            "        (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
            "      )\n",
            "      (3-23): 21 x Qwen3DecoderLayer(\n",
            "        (self_attn): Qwen3Attention(\n",
            "          (q_proj): Linear4bit(in_features=1024, out_features=2048, bias=False)\n",
            "          (k_proj): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
            "          (v_proj): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
            "          (o_proj): Linear4bit(in_features=2048, out_features=1024, bias=False)\n",
            "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
            "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
            "          (rotary_emb): LlamaRotaryEmbedding()\n",
            "        )\n",
            "        (mlp): Qwen3MLP(\n",
            "          (gate_proj): Linear4bit(in_features=1024, out_features=3072, bias=False)\n",
            "          (up_proj): Linear4bit(in_features=1024, out_features=3072, bias=False)\n",
            "          (down_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
            "        (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
            "      )\n",
            "      (24): Qwen3DecoderLayer(\n",
            "        (self_attn): Qwen3Attention(\n",
            "          (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
            "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
            "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
            "          (rotary_emb): LlamaRotaryEmbedding()\n",
            "        )\n",
            "        (mlp): Qwen3MLP(\n",
            "          (gate_proj): Linear4bit(in_features=1024, out_features=3072, bias=False)\n",
            "          (up_proj): Linear4bit(in_features=1024, out_features=3072, bias=False)\n",
            "          (down_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
            "        (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
            "      )\n",
            "      (25-26): 2 x Qwen3DecoderLayer(\n",
            "        (self_attn): Qwen3Attention(\n",
            "          (q_proj): Linear4bit(in_features=1024, out_features=2048, bias=False)\n",
            "          (k_proj): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
            "          (v_proj): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
            "          (o_proj): Linear4bit(in_features=2048, out_features=1024, bias=False)\n",
            "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
            "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
            "          (rotary_emb): LlamaRotaryEmbedding()\n",
            "        )\n",
            "        (mlp): Qwen3MLP(\n",
            "          (gate_proj): Linear4bit(in_features=1024, out_features=3072, bias=False)\n",
            "          (up_proj): Linear4bit(in_features=1024, out_features=3072, bias=False)\n",
            "          (down_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
            "        (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
            "      )\n",
            "      (27): Qwen3DecoderLayer(\n",
            "        (self_attn): Qwen3Attention(\n",
            "          (q_proj): Linear4bit(in_features=1024, out_features=2048, bias=False)\n",
            "          (k_proj): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
            "          (v_proj): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
            "          (o_proj): Linear4bit(in_features=2048, out_features=1024, bias=False)\n",
            "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
            "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
            "          (rotary_emb): LlamaRotaryEmbedding()\n",
            "        )\n",
            "        (mlp): Qwen3MLP(\n",
            "          (gate_proj): Linear4bit(in_features=1024, out_features=3072, bias=False)\n",
            "          (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
            "          (down_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
            "        (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
            "      )\n",
            "    )\n",
            "    (norm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
            "    (rotary_emb): LlamaRotaryEmbedding()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=1024, out_features=151936, bias=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Archive the packages"
      ],
      "metadata": {
        "id": "iW5hFKTnyxth"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -czvf unsloth_env124.tar.gz ./.venv"
      ],
      "metadata": {
        "id": "UKLFz1I7wtfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sDDTAThtyjdK",
        "outputId": "2fe41036-0c5e-459b-ca49-ab56844bf499"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv unsloth_env124.tar.gz /content/gdrive/MyDrive/Unsloth_env/"
      ],
      "metadata": {
        "id": "_m4b_P751b7c"
      },
      "execution_count": 9,
      "outputs": []
    }
  ]
}