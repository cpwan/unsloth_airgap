{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LM5uT2yTRK0O",
        "outputId": "1e28ec39-d82d-4cfe-d265-b0268abc9d89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Jul 29 11:27:11 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   61C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HpdjNJhe2-2t",
        "outputId": "d2c7238e-b09e-4663-9520-ea562ed4c1d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZKx3tlYSnzI"
      },
      "outputs": [],
      "source": [
        "!tar xzvf /content/gdrive/MyDrive/Unsloth_env/unsloth_env.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "YDwyqtt85C6M"
      },
      "outputs": [],
      "source": [
        "script = \"\"\"\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Qwen3-0.6B\",\n",
        "    max_seq_length = 2048,   # Context length - can be longer, but uses more memory\n",
        "    load_in_4bit = True,     # 4bit uses much less memory\n",
        "    load_in_8bit = False,    # A bit more accurate, uses 2x memory\n",
        "    full_finetuning = False, # We have full finetuning now!\n",
        "    # token = \"hf_...\",      # use one if using gated models\n",
        ")\n",
        "print(model)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ux4vsQTwTdyk",
        "outputId": "fa067e59-ebb5-4adb-e519-b7903e6e10e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
            "==((====))==  Unsloth 2025.7.11: Fast Qwen3 patching. Transformers: 4.54.0.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.7.1+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.3.1\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.31.post1. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "model.safetensors: 100% 576M/576M [00:05<00:00, 107MB/s]\n",
            "generation_config.json: 100% 237/237 [00:00<00:00, 1.27MB/s]\n",
            "tokenizer_config.json: 10.5kB [00:00, 44.1MB/s]\n",
            "vocab.json: 2.78MB [00:00, 104MB/s]\n",
            "merges.txt: 1.67MB [00:00, 132MB/s]\n",
            "added_tokens.json: 100% 707/707 [00:00<00:00, 5.05MB/s]\n",
            "special_tokens_map.json: 100% 614/614 [00:00<00:00, 4.10MB/s]\n",
            "tokenizer.json: 100% 11.4M/11.4M [00:00<00:00, 19.2MB/s]\n",
            "chat_template.jinja: 4.91kB [00:00, 22.8MB/s]\n",
            "Qwen3ForCausalLM(\n",
            "  (model): Qwen3Model(\n",
            "    (embed_tokens): Embedding(151936, 1024, padding_idx=151654)\n",
            "    (layers): ModuleList(\n",
            "      (0): Qwen3DecoderLayer(\n",
            "        (self_attn): Qwen3Attention(\n",
            "          (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
            "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
            "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
            "          (rotary_emb): LlamaRotaryEmbedding()\n",
            "        )\n",
            "        (mlp): Qwen3MLP(\n",
            "          (gate_proj): Linear4bit(in_features=1024, out_features=3072, bias=False)\n",
            "          (up_proj): Linear4bit(in_features=1024, out_features=3072, bias=False)\n",
            "          (down_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
            "        (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
            "      )\n",
            "      (1): Qwen3DecoderLayer(\n",
            "        (self_attn): Qwen3Attention(\n",
            "          (q_proj): Linear4bit(in_features=1024, out_features=2048, bias=False)\n",
            "          (k_proj): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
            "          (v_proj): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
            "          (o_proj): Linear4bit(in_features=2048, out_features=1024, bias=False)\n",
            "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
            "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
            "          (rotary_emb): LlamaRotaryEmbedding()\n",
            "        )\n",
            "        (mlp): Qwen3MLP(\n",
            "          (gate_proj): Linear4bit(in_features=1024, out_features=3072, bias=False)\n",
            "          (up_proj): Linear4bit(in_features=1024, out_features=3072, bias=False)\n",
            "          (down_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
            "        (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
            "      )\n",
            "      (2): Qwen3DecoderLayer(\n",
            "        (self_attn): Qwen3Attention(\n",
            "          (q_proj): Linear4bit(in_features=1024, out_features=2048, bias=False)\n",
            "          (k_proj): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
            "          (v_proj): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
            "          (o_proj): Linear4bit(in_features=2048, out_features=1024, bias=False)\n",
            "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
            "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
            "          (rotary_emb): LlamaRotaryEmbedding()\n",
            "        )\n",
            "        (mlp): Qwen3MLP(\n",
            "          (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
            "          (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
            "          (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
            "        (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
            "      )\n",
            "      (3-23): 21 x Qwen3DecoderLayer(\n",
            "        (self_attn): Qwen3Attention(\n",
            "          (q_proj): Linear4bit(in_features=1024, out_features=2048, bias=False)\n",
            "          (k_proj): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
            "          (v_proj): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
            "          (o_proj): Linear4bit(in_features=2048, out_features=1024, bias=False)\n",
            "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
            "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
            "          (rotary_emb): LlamaRotaryEmbedding()\n",
            "        )\n",
            "        (mlp): Qwen3MLP(\n",
            "          (gate_proj): Linear4bit(in_features=1024, out_features=3072, bias=False)\n",
            "          (up_proj): Linear4bit(in_features=1024, out_features=3072, bias=False)\n",
            "          (down_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
            "        (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
            "      )\n",
            "      (24): Qwen3DecoderLayer(\n",
            "        (self_attn): Qwen3Attention(\n",
            "          (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
            "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
            "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
            "          (rotary_emb): LlamaRotaryEmbedding()\n",
            "        )\n",
            "        (mlp): Qwen3MLP(\n",
            "          (gate_proj): Linear4bit(in_features=1024, out_features=3072, bias=False)\n",
            "          (up_proj): Linear4bit(in_features=1024, out_features=3072, bias=False)\n",
            "          (down_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
            "        (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
            "      )\n",
            "      (25-26): 2 x Qwen3DecoderLayer(\n",
            "        (self_attn): Qwen3Attention(\n",
            "          (q_proj): Linear4bit(in_features=1024, out_features=2048, bias=False)\n",
            "          (k_proj): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
            "          (v_proj): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
            "          (o_proj): Linear4bit(in_features=2048, out_features=1024, bias=False)\n",
            "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
            "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
            "          (rotary_emb): LlamaRotaryEmbedding()\n",
            "        )\n",
            "        (mlp): Qwen3MLP(\n",
            "          (gate_proj): Linear4bit(in_features=1024, out_features=3072, bias=False)\n",
            "          (up_proj): Linear4bit(in_features=1024, out_features=3072, bias=False)\n",
            "          (down_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
            "        (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
            "      )\n",
            "      (27): Qwen3DecoderLayer(\n",
            "        (self_attn): Qwen3Attention(\n",
            "          (q_proj): Linear4bit(in_features=1024, out_features=2048, bias=False)\n",
            "          (k_proj): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
            "          (v_proj): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
            "          (o_proj): Linear4bit(in_features=2048, out_features=1024, bias=False)\n",
            "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
            "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
            "          (rotary_emb): LlamaRotaryEmbedding()\n",
            "        )\n",
            "        (mlp): Qwen3MLP(\n",
            "          (gate_proj): Linear4bit(in_features=1024, out_features=3072, bias=False)\n",
            "          (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
            "          (down_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
            "        (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
            "      )\n",
            "    )\n",
            "    (norm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
            "    (rotary_emb): LlamaRotaryEmbedding()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=1024, out_features=151936, bias=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "!./.venv/bin/python -c '{script}'"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}